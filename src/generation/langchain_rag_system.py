# src/generation/langchain_rag_system.py
"""
åŸºäºLangChain LCELçš„ç°ä»£åŒ–RAGç³»ç»Ÿ
å®ç°2024-2025å¹´æœ€æ–°çš„RAGæ¶æ„å’Œä¼˜åŒ–æŠ€æœ¯
"""

from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
import asyncio
import time
import json
from loguru import logger

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain_core.runnables import (
    RunnablePassthrough, 
    RunnableParallel, 
    RunnableLambda,
    RunnableSequence
)
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_qdrant import QdrantVectorStore

# RAGä¸“ç”¨ç»„ä»¶
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

# æœ¬åœ°ç»„ä»¶å¯¼å…¥
from .hybrid_retriever import HybridRetriever, EnhancedDocument, RetrievalMetadata
from src.retrieval.query_intelligence import QueryIntelligenceEngine
from src.knowledge_graph.kg_retriever import KnowledgeGraphRetriever
from configs.config import config


@dataclass
class LangChainRAGResult:
    """LangChain RAGç³»ç»Ÿç»“æœ"""
    answer: str
    source_documents: List[EnhancedDocument]
    retrieval_metadata: Dict[str, Any]
    generation_metadata: Dict[str, Any]
    total_time: float
    confidence_score: float


class QueryComplexityAnalyzer:
    """æŸ¥è¯¢å¤æ‚åº¦åˆ†æå™¨"""
    
    def __init__(self, llm):
        self.llm = llm
        self.complexity_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢å¤æ‚åº¦åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢çš„å¤æ‚åº¦å¹¶è¿”å›JSONæ ¼å¼ç»“æœã€‚

å¤æ‚åº¦ç­‰çº§ï¼š
- SIMPLE: ç®€å•äº‹å®æŸ¥è¯¢ï¼Œç›´æ¥ç­”æ¡ˆ
- MEDIUM: éœ€è¦ä¸€å®šæ¨ç†çš„æŸ¥è¯¢
- COMPLEX: å¤šæ­¥æ¨ç†ã€æ¯”è¾ƒåˆ†æç­‰
- CRITICAL: éœ€è¦æ·±åº¦åˆ†æã€åˆ›æ„æ€è€ƒçš„æŸ¥è¯¢

è¯·è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼š
{{
    "complexity": "SIMPLE|MEDIUM|COMPLEX|CRITICAL",
    "reasoning": "åˆ†æç†ç”±",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "query_type": "factual|comparative|explanatory|procedural|creative"
}}"""),
            ("human", "æŸ¥è¯¢: {query}")
        ])
        
        self.analysis_chain = (
            self.complexity_prompt 
            | self.llm 
            | JsonOutputParser()
        )
    
    async def analyze(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        try:
            result = await self.analysis_chain.ainvoke({"query": query})
            return result
        except Exception as e:
            logger.warning(f"Query complexity analysis failed: {e}")
            return {
                "complexity": "MEDIUM",
                "reasoning": "åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤æ‚åº¦",
                "keywords": query.split()[:5],
                "query_type": "factual"
            }


class ContextualRetriever:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢å™¨"""
    
    def __init__(self, hybrid_retriever: HybridRetriever, reranker=None):
        self.hybrid_retriever = hybrid_retriever
        self.reranker = reranker
        
        # å¦‚æœæœ‰é‡æ’åºå™¨ï¼Œåˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
        if reranker:
            self.compression_retriever = ContextualCompressionRetriever(
                base_compressor=reranker,
                base_retriever=hybrid_retriever
            )
        else:
            self.compression_retriever = hybrid_retriever
    
    async def retrieve(self, query: str, complexity_info: Dict) -> List[EnhancedDocument]:
        """åŸºäºæŸ¥è¯¢å¤æ‚åº¦çš„è‡ªé€‚åº”æ£€ç´¢"""
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ£€ç´¢å‚æ•°
        k = self._get_k_by_complexity(complexity_info["complexity"])
        self.hybrid_retriever.k = k
        
        # æ‰§è¡Œæ£€ç´¢
        docs = await self.compression_retriever._aget_relevant_documents(query)
        
        # å¦‚æœæ˜¯å¤æ‚æŸ¥è¯¢ï¼Œå¯èƒ½éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
        if complexity_info["complexity"] in ["COMPLEX", "CRITICAL"] and len(docs) < k:
            # å°è¯•ä½¿ç”¨æ›´å®½æ³›çš„æœç´¢
            expanded_query = f"{query} {' '.join(complexity_info.get('keywords', []))}"
            additional_docs = await self.hybrid_retriever._aget_relevant_documents(expanded_query)
            
            # åˆå¹¶å»é‡
            seen_ids = {self._get_doc_id(doc) for doc in docs}
            for doc in additional_docs:
                if self._get_doc_id(doc) not in seen_ids:
                    docs.append(doc)
                    if len(docs) >= k:
                        break
        
        return docs[:k]
    
    def _get_k_by_complexity(self, complexity: str) -> int:
        """æ ¹æ®å¤æ‚åº¦ç¡®å®šæ£€ç´¢æ•°é‡"""
        k_map = {
            "SIMPLE": 5,
            "MEDIUM": 10,
            "COMPLEX": 15,
            "CRITICAL": 20
        }
        return k_map.get(complexity, 10)
    
    def _get_doc_id(self, doc: EnhancedDocument) -> str:
        """è·å–æ–‡æ¡£ID"""
        import hashlib
        return hashlib.md5(doc.page_content.encode()).hexdigest()


class ResponseGenerator:
    """å“åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, llm, local_llm=None):
        self.llm = llm  # ä¸»è¦LLMï¼ˆé€šå¸¸æ˜¯APIæ¨¡å‹ï¼‰
        self.local_llm = local_llm  # æœ¬åœ°LLM
        
        # ç”Ÿæˆæç¤ºæ¨¡æ¿
        self.generation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”æŠ€æœ¯ç›¸å…³é—®é¢˜ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ç­”æ¡ˆè¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†
2. ä¼˜å…ˆä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
3. å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
4. å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£æ¥æº
5. ä¿æŒä¸“ä¸šæ€§å’Œå®¢è§‚æ€§

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

æ£€ç´¢å…ƒæ•°æ®ï¼š
{retrieval_info}"""),
            ("human", "é—®é¢˜ï¼š{query}")
        ])
        
        # è‡ªæˆ‘è¯„ä¼°æç¤º
        self.self_eval_prompt = ChatPromptTemplate.from_messages([
            ("system", """è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ã€‚è¿”å›JSONæ ¼å¼ï¼š
{{
    "relevance_score": 0.8,
    "completeness_score": 0.9,
    "accuracy_score": 0.85,
    "overall_confidence": 0.85,
    "reasoning": "è¯„ä¼°ç†ç”±"
}}

é—®é¢˜: {query}
å›ç­”: {answer}
å‚è€ƒä¸Šä¸‹æ–‡: {context}"""),
            ("human", "è¯·è¯„ä¼°ä¸Šè¿°å›ç­”çš„è´¨é‡")
        ])
    
    async def generate(self, query: str, docs: List[EnhancedDocument], complexity_info: Dict) -> LangChainRAGResult:
        """ç”Ÿæˆå›ç­”"""
        start_time = time.time()
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        selected_llm = self._select_model(complexity_info["complexity"])
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = self._format_context(docs)
        retrieval_info = self._format_retrieval_info(docs)
        
        # ç”Ÿæˆé“¾
        generation_chain = (
            self.generation_prompt 
            | selected_llm 
            | StrOutputParser()
        )
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = await generation_chain.ainvoke({
            "query": query,
            "context": context,
            "retrieval_info": retrieval_info
        })
        
        # è‡ªæˆ‘è¯„ä¼°
        eval_chain = (
            self.self_eval_prompt 
            | selected_llm 
            | JsonOutputParser()
        )
        
        try:
            evaluation = await eval_chain.ainvoke({
                "query": query,
                "answer": answer,
                "context": context[:1000]  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºé™åˆ¶
            })
            confidence_score = evaluation.get("overall_confidence", 0.7)
        except Exception as e:
            logger.warning(f"Self-evaluation failed: {e}")
            confidence_score = 0.7  # é»˜è®¤ç½®ä¿¡åº¦
            evaluation = {"reasoning": "è‡ªæˆ‘è¯„ä¼°å¤±è´¥"}
        
        total_time = time.time() - start_time
        
        # æ„å»ºç»“æœ
        result = LangChainRAGResult(
            answer=answer,
            source_documents=docs,
            retrieval_metadata={
                "num_sources": len(docs),
                "retrieval_methods": self._get_retrieval_methods(docs),
                "avg_relevance": self._calculate_avg_relevance(docs)
            },
            generation_metadata={
                "model_used": selected_llm.__class__.__name__,
                "complexity": complexity_info["complexity"],
                "evaluation": evaluation
            },
            total_time=total_time,
            confidence_score=confidence_score
        )
        
        return result
    
    def _select_model(self, complexity: str):
        """æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        if complexity in ["SIMPLE", "MEDIUM"] and self.local_llm:
            return self.local_llm
        return self.llm
    
    def _format_context(self, docs: List[EnhancedDocument]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('title', doc.metadata.get('source', f'æ–‡æ¡£{i}'))
            content = doc.page_content
            context_parts.append(f"[æ¥æº{i}: {source}]\n{content}\n")
        
        return "\n".join(context_parts)
    
    def _format_retrieval_info(self, docs: List[EnhancedDocument]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ä¿¡æ¯"""
        info_parts = []
        for i, doc in enumerate(docs, 1):
            if doc.retrieval_metadata:
                method = doc.retrieval_metadata.retrieval_method
                score = doc.retrieval_metadata.confidence_score
                info_parts.append(f"æ–‡æ¡£{i}: {method}æ£€ç´¢, ç›¸å…³æ€§={score:.3f}")
        
        return "; ".join(info_parts)
    
    def _get_retrieval_methods(self, docs: List[EnhancedDocument]) -> List[str]:
        """è·å–ä½¿ç”¨çš„æ£€ç´¢æ–¹æ³•"""
        methods = set()
        for doc in docs:
            if doc.retrieval_metadata:
                methods.add(doc.retrieval_metadata.retrieval_method)
        return list(methods)
    
    def _calculate_avg_relevance(self, docs: List[EnhancedDocument]) -> float:
        """è®¡ç®—å¹³å‡ç›¸å…³æ€§"""
        if not docs:
            return 0.0
        
        scores = []
        for doc in docs:
            if doc.retrieval_metadata:
                scores.append(doc.retrieval_metadata.confidence_score)
        
        return sum(scores) / len(scores) if scores else 0.0


class LangChainRAGSystem:
    """åŸºäºLangChain LCELçš„ç°ä»£RAGç³»ç»Ÿ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        logger.info("ğŸš€ Initializing LangChain RAG System...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = self._init_llm()
        self.local_llm = self._init_local_llm()
        self.embeddings = self._init_embeddings()
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = self._init_vector_store()
        
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ï¼ˆå¦‚æœé…ç½®ï¼‰
        self.kg_retriever = self._init_kg_retriever()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self.reranker = self._init_reranker()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.query_analyzer = QueryComplexityAnalyzer(self.llm)
        self.query_intelligence = QueryIntelligenceEngine(config)
        
        # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼ˆéœ€è¦æ–‡æ¡£åˆ—è¡¨ï¼Œç¨åè®¾ç½®ï¼‰
        self.hybrid_retriever = None
        self.contextual_retriever = None
        self.response_generator = ResponseGenerator(self.llm, self.local_llm)
        
        # æ„å»ºLCELé“¾
        self._build_rag_chain()
        
        logger.success("âœ… LangChain RAG System initialized!")
    
    def _init_llm(self):
        """åˆå§‹åŒ–ä¸»è¦LLM"""
        if self.config.get('openai_api_key'):
            return ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.1,
                api_key=self.config['openai_api_key']
            )
        else:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä½œä¸ºå¤‡é€‰
            return self._init_local_llm()
    
    def _init_local_llm(self):
        """åˆå§‹åŒ–æœ¬åœ°LLM"""
        try:
            # è¿™é‡Œå¯ä»¥é›†æˆæœ¬åœ°æ¨¡å‹ï¼Œå¦‚Qwen2
            # æš‚æ—¶è¿”å›Noneï¼Œè¡¨ç¤ºæœªé…ç½®
            return None
        except Exception as e:
            logger.warning(f"Local LLM initialization failed: {e}")
            return None
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        if self.config.get('openai_api_key'):
            return OpenAIEmbeddings(api_key=self.config['openai_api_key'])
        else:
            # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
            from langchain_community.embeddings import HuggingFaceEmbeddings
            return HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'}
            )
    
    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            from qdrant_client import QdrantClient
            
            client = QdrantClient(
                host=self.config.get('qdrant_host', 'localhost'),
                port=self.config.get('qdrant_port', 6333)
            )
            
            return QdrantVectorStore(
                client=client,
                collection_name=self.config.get('collection_name', 'documents'),
                embeddings=self.embeddings
            )
        except Exception as e:
            logger.error(f"Vector store initialization failed: {e}")
            return None
    
    def _init_kg_retriever(self):
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨"""
        if self.config.get('enable_knowledge_graph', False):
            try:
                return KnowledgeGraphRetriever(self.config)
            except Exception as e:
                logger.warning(f"Knowledge graph retriever initialization failed: {e}")
        return None
    
    def _init_reranker(self):
        """åˆå§‹åŒ–é‡æ’åºå™¨"""
        try:
            if self.config.get('cohere_api_key'):
                return CohereRerank(
                    cohere_api_key=self.config['cohere_api_key'],
                    top_n=10
                )
        except Exception as e:
            logger.warning(f"Reranker initialization failed: {e}")
        return None
    
    def _build_rag_chain(self):
        """æ„å»ºLCEL RAGé“¾"""
        
        # æŸ¥è¯¢åˆ†æå’Œå¢å¼ºæ­¥éª¤
        query_processing_chain = RunnableParallel({
            "original_query": RunnablePassthrough(),
            "complexity_analysis": RunnableLambda(self._analyze_query_complexity),
            "enhanced_queries": RunnableLambda(self._enhance_query)
        })
        
        # æ£€ç´¢æ­¥éª¤
        retrieval_chain = RunnableLambda(self._contextual_retrieve)
        
        # ç”Ÿæˆæ­¥éª¤
        generation_chain = RunnableLambda(self._generate_response)
        
        # å®Œæ•´çš„RAGé“¾
        self.rag_chain = (
            query_processing_chain
            | retrieval_chain
            | generation_chain
        )
        
        logger.info("âœ… LCEL RAG chain built successfully")
    
    async def _analyze_query_complexity(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        return await self.query_analyzer.analyze(query)
    
    async def _enhance_query(self, query: str) -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢"""
        try:
            return await self.query_intelligence.process_query(query)
        except Exception as e:
            logger.warning(f"Query enhancement failed: {e}")
            return {"enhanced_query": query, "sub_questions": [], "hyde_document": None}
    
    async def _contextual_retrieve(self, processed_input: Dict) -> Dict:
        """ä¸Šä¸‹æ–‡æ£€ç´¢"""
        query = processed_input["original_query"]
        complexity = processed_input["complexity_analysis"]
        
        # ç¡®ä¿æ··åˆæ£€ç´¢å™¨å·²åˆå§‹åŒ–
        if not self.hybrid_retriever:
            await self._init_hybrid_retriever()
        
        if not self.contextual_retriever:
            return processed_input  # è¿”å›åŸè¾“å…¥ï¼Œé¿å…é“¾ä¸­æ–­
        
        # æ‰§è¡Œæ£€ç´¢
        docs = await self.contextual_retriever.retrieve(query, complexity)
        
        return {
            **processed_input,
            "retrieved_docs": docs
        }
    
    async def _generate_response(self, retrieval_result: Dict) -> LangChainRAGResult:
        """ç”Ÿæˆå“åº”"""
        query = retrieval_result["original_query"]
        docs = retrieval_result.get("retrieved_docs", [])
        complexity = retrieval_result["complexity_analysis"]
        
        return await self.response_generator.generate(query, docs, complexity)
    
    async def _init_hybrid_retriever(self):
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨"""
        try:
            # è·å–æ–‡æ¡£åˆ—è¡¨ç”¨äºBM25
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„å‘é‡å­˜å‚¨å®ç°æ¥è·å–æ–‡æ¡£
            documents = await self._get_documents_from_vector_store()
            
            self.hybrid_retriever = HybridRetriever(
                vector_store=self.vector_store,
                documents=documents,
                kg_retriever=self.kg_retriever,
                vector_weight=0.5,
                bm25_weight=0.3,
                kg_weight=0.2
            )
            
            self.contextual_retriever = ContextualRetriever(
                self.hybrid_retriever,
                self.reranker
            )
            
            logger.info("âœ… Hybrid retriever initialized")
            
        except Exception as e:
            logger.error(f"Hybrid retriever initialization failed: {e}")
            self.hybrid_retriever = None
            self.contextual_retriever = None
    
    async def _get_documents_from_vector_store(self) -> List[Document]:
        """ä»å‘é‡å­˜å‚¨è·å–æ–‡æ¡£ï¼ˆç”¨äºBM25ç´¢å¼•ï¼‰"""
        try:
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”è¯¥ä»å‘é‡å­˜å‚¨ä¸­è·å–æ‰€æœ‰æ–‡æ¡£
            # ç”±äºQdrantçš„é™åˆ¶ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦åˆ†æ‰¹è·å–
            
            # ä¸´æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®ç°å…·ä½“é€»è¾‘
            return []
        except Exception as e:
            logger.warning(f"Failed to get documents from vector store: {e}")
            return []
    
    async def query(self, user_query: str, **kwargs) -> LangChainRAGResult:
        """æ‰§è¡ŒRAGæŸ¥è¯¢"""
        logger.info(f"ğŸ” Processing query: {user_query[:100]}...")
        
        try:
            start_time = time.time()
            
            # æ‰§è¡ŒRAGé“¾
            result = await self.rag_chain.ainvoke(user_query)
            
            # æ›´æ–°æ€»æ—¶é—´
            result.total_time = time.time() - start_time
            
            logger.success(f"âœ… Query completed in {result.total_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Query failed: {e}")
            
            # è¿”å›é”™è¯¯ç»“æœ
            return LangChainRAGResult(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                source_documents=[],
                retrieval_metadata={},
                generation_metadata={},
                total_time=time.time() - start_time if 'start_time' in locals() else 0,
                confidence_score=0.0
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "components_initialized": {
                "vector_store": self.vector_store is not None,
                "hybrid_retriever": self.hybrid_retriever is not None,
                "kg_retriever": self.kg_retriever is not None,
                "reranker": self.reranker is not None,
                "local_llm": self.local_llm is not None
            },
            "config": {
                "enable_kg": self.config.get('enable_knowledge_graph', False),
                "enable_reranker": self.reranker is not None
            }
        }