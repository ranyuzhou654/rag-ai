# src/retrieval/enhanced_agentic_rag.py
"""
å¢å¼ºçš„æ™ºèƒ½ä½“RAGç³»ç»Ÿ
åŸºäºimprovements.mdçš„å»ºè®®ï¼Œå®ç°ç»†åŒ–è¯„ä¼°ç»´åº¦å’Œå…·è±¡åŒ–æ”¹è¿›å»ºè®®
"""

from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import asyncio
import time
import json
from loguru import logger

# LangChainç»„ä»¶
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnableParallel

# æœ¬åœ°ç»„ä»¶
from .hybrid_retriever import HybridRetriever, EnhancedDocument


class AgenticDecision(Enum):
    """æ™ºèƒ½ä½“å†³ç­–ç±»å‹"""
    PROCEED = "proceed"           # ç»§ç»­ä½¿ç”¨å½“å‰ç»“æœ
    RETRY = "retry"              # é‡æ–°æ£€ç´¢
    EXPAND_QUERY = "expand_query"  # æ‰©å±•æŸ¥è¯¢
    SEEK_MORE = "seek_more"      # å¯»æ‰¾æ›´å¤šä¿¡æ¯
    REFINE_QUERY = "refine_query" # ç²¾ç‚¼æŸ¥è¯¢


@dataclass
class DetailedEvaluation:
    """è¯¦ç»†è¯„ä¼°ç»“æœ"""
    # å››ä¸ªæ ¸å¿ƒç»´åº¦
    relevance_score: float      # ç›¸å…³æ€§ (0-1)
    completeness_score: float   # å®Œæ•´æ€§ (0-1)
    novelty_score: float        # æ–°é¢–æ€§ (0-1)
    authority_score: float      # æƒå¨æ€§ (0-1)
    
    # ç»¼åˆè¯„åˆ†
    overall_score: float
    
    # è¯„ä¼°è¯¦æƒ…
    evaluation_reasoning: str
    specific_issues: List[str]
    
    # å…·è±¡åŒ–æ”¹è¿›å»ºè®®
    improvement_suggestions: Dict[str, str]
    
    # å†³ç­–
    recommended_action: AgenticDecision
    confidence_in_decision: float


@dataclass
class RetrievalIteration:
    """æ£€ç´¢è¿­ä»£è®°å½•"""
    iteration_number: int
    query_used: str
    documents_retrieved: List[EnhancedDocument]
    evaluation: DetailedEvaluation
    action_taken: AgenticDecision
    processing_time: float


@dataclass
class AgenticRetrievalResult:
    """æ™ºèƒ½ä½“æ£€ç´¢ç»“æœ"""
    final_documents: List[EnhancedDocument]
    all_iterations: List[RetrievalIteration]
    total_iterations: int
    success: bool
    final_evaluation: DetailedEvaluation
    total_time: float
    
    # é¢å¤–ä¿¡æ¯
    query_evolution: List[str]
    strategies_used: List[str]
    performance_metrics: Dict[str, Any]


class DetailedEvaluator:
    """è¯¦ç»†è¯„ä¼°å™¨ - å®ç°å››ç»´è¯„ä¼°ä½“ç³»"""
    
    def __init__(self, llm, config: Dict[str, Any]):
        self.llm = llm
        self.config = config
        
        # è¯„ä¼°æç¤ºæ¨¡æ¿
        self.evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ£€ç´¢è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹æ£€ç´¢ç»“æœè¿›è¡Œè¯¦ç»†è¯„ä¼°ï¼Œå¹¶è¿”å›JSONæ ¼å¼ç»“æœã€‚

è¯„ä¼°ç»´åº¦è¯´æ˜ï¼š
1. ç›¸å…³æ€§(Relevance): æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³ç¨‹åº¦
2. å®Œæ•´æ€§(Completeness): æ£€ç´¢ç»“æœæ˜¯å¦è¶³ä»¥å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜
3. æ–°é¢–æ€§(Novelty): æ£€ç´¢ç»“æœæ˜¯å¦åŒ…å«å¤šæ ·åŒ–çš„ä¿¡æ¯ï¼Œé¿å…é‡å¤
4. æƒå¨æ€§(Authority): æ£€ç´¢ç»“æœçš„æ¥æºå¯ä¿¡åº¦å’Œä¸“ä¸šæ€§

å†³ç­–ç±»å‹è¯´æ˜ï¼š
- proceed: å½“å‰ç»“æœæ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥ç»§ç»­
- retry: ç»“æœè´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°æ£€ç´¢
- expand_query: ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œéœ€è¦æ‰©å±•æŸ¥è¯¢èŒƒå›´
- seek_more: éœ€è¦å¯»æ‰¾æ›´å¤šè¡¥å……ä¿¡æ¯
- refine_query: æŸ¥è¯¢è¿‡äºå®½æ³›ï¼Œéœ€è¦ç²¾ç‚¼

è¯·è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼š
{{
    "relevance_score": 0.8,
    "completeness_score": 0.7,
    "novelty_score": 0.6,
    "authority_score": 0.9,
    "overall_score": 0.75,
    "evaluation_reasoning": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±",
    "specific_issues": ["å…·ä½“é—®é¢˜1", "å…·ä½“é—®é¢˜2"],
    "improvement_suggestions": {{
        "query_modification": "å…·ä½“çš„æŸ¥è¯¢ä¿®æ”¹å»ºè®®",
        "search_strategy": "æœç´¢ç­–ç•¥å»ºè®®",
        "additional_keywords": "å»ºè®®æ·»åŠ çš„å…³é”®è¯",
        "time_constraint": "æ—¶é—´èŒƒå›´å»ºè®®",
        "domain_focus": "é¢†åŸŸèšç„¦å»ºè®®"
    }},
    "recommended_action": "proceed|retry|expand_query|seek_more|refine_query",
    "confidence_in_decision": 0.85
}}

åŸå§‹æŸ¥è¯¢ï¼š{query}
æ£€ç´¢å†å²ï¼š{retrieval_history}"""),
            ("human", """è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢ç»“æœï¼š

æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡ï¼š{num_docs}
æ–‡æ¡£ä¿¡æ¯ï¼š
{document_info}

æ£€ç´¢ç­–ç•¥ï¼š{retrieval_methods}
å¹³å‡ç›¸å…³æ€§åˆ†æ•°ï¼š{avg_relevance}
""")
        ])
        
        self.evaluation_chain = (
            self.evaluation_prompt 
            | self.llm 
            | JsonOutputParser()
        )
    
    async def evaluate_retrieval(
        self,
        query: str,
        documents: List[EnhancedDocument],
        retrieval_history: List[RetrievalIteration],
        retrieval_methods: List[str]
    ) -> DetailedEvaluation:
        """æ‰§è¡Œè¯¦ç»†è¯„ä¼°"""
        
        try:
            # å‡†å¤‡æ–‡æ¡£ä¿¡æ¯
            document_info = self._format_document_info(documents)
            avg_relevance = self._calculate_average_relevance(documents)
            
            # æ ¼å¼åŒ–æ£€ç´¢å†å²
            history_summary = self._format_retrieval_history(retrieval_history)
            
            # æ‰§è¡Œè¯„ä¼°
            evaluation_result = await self.evaluation_chain.ainvoke({
                "query": query,
                "num_docs": len(documents),
                "document_info": document_info,
                "retrieval_methods": retrieval_methods,
                "avg_relevance": avg_relevance,
                "retrieval_history": history_summary
            })
            
            # æ„å»ºè¯¦ç»†è¯„ä¼°å¯¹è±¡
            return DetailedEvaluation(
                relevance_score=evaluation_result.get("relevance_score", 0.0),
                completeness_score=evaluation_result.get("completeness_score", 0.0),
                novelty_score=evaluation_result.get("novelty_score", 0.0),
                authority_score=evaluation_result.get("authority_score", 0.0),
                overall_score=evaluation_result.get("overall_score", 0.0),
                evaluation_reasoning=evaluation_result.get("evaluation_reasoning", ""),
                specific_issues=evaluation_result.get("specific_issues", []),
                improvement_suggestions=evaluation_result.get("improvement_suggestions", {}),
                recommended_action=AgenticDecision(evaluation_result.get("recommended_action", "proceed")),
                confidence_in_decision=evaluation_result.get("confidence_in_decision", 0.5)
            )
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            
            # è¿”å›é»˜è®¤è¯„ä¼°
            return self._create_fallback_evaluation(documents)
    
    def _format_document_info(self, documents: List[EnhancedDocument]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ä¿¡æ¯"""
        doc_info = []
        
        for i, doc in enumerate(documents, 1):
            title = doc.metadata.get('title', f'æ–‡æ¡£{i}')
            source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
            length = len(doc.page_content)
            
            retrieval_method = "æœªçŸ¥"
            confidence = 0.0
            if doc.retrieval_metadata:
                retrieval_method = doc.retrieval_metadata.retrieval_method
                confidence = doc.retrieval_metadata.confidence_score
            
            doc_info.append(
                f"æ–‡æ¡£{i}: {title} | æ¥æº: {source} | é•¿åº¦: {length}å­—ç¬¦ | "
                f"æ£€ç´¢æ–¹å¼: {retrieval_method} | ç½®ä¿¡åº¦: {confidence:.3f}"
            )
        
        return "\n".join(doc_info)
    
    def _calculate_average_relevance(self, documents: List[EnhancedDocument]) -> float:
        """è®¡ç®—å¹³å‡ç›¸å…³æ€§"""
        if not documents:
            return 0.0
        
        scores = []
        for doc in documents:
            if doc.retrieval_metadata:
                scores.append(doc.retrieval_metadata.confidence_score)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _format_retrieval_history(self, history: List[RetrievalIteration]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢å†å²"""
        if not history:
            return "æ— å†å²è®°å½•"
        
        history_items = []
        for iteration in history:
            history_items.append(
                f"ç¬¬{iteration.iteration_number}è½®: æŸ¥è¯¢='{iteration.query_used}', "
                f"ç»“æœ={len(iteration.documents_retrieved)}ç¯‡, "
                f"è¯„åˆ†={iteration.evaluation.overall_score:.2f}, "
                f"å†³ç­–={iteration.action_taken.value}"
            )
        
        return " -> ".join(history_items)
    
    def _create_fallback_evaluation(self, documents: List[EnhancedDocument]) -> DetailedEvaluation:
        """åˆ›å»ºé»˜è®¤è¯„ä¼°ï¼ˆå½“LLMè¯„ä¼°å¤±è´¥æ—¶ï¼‰"""
        
        # åŸºäºç®€å•è§„åˆ™çš„è¯„ä¼°
        num_docs = len(documents)
        avg_relevance = self._calculate_average_relevance(documents)
        
        # ç®€å•è¯„åˆ†é€»è¾‘
        relevance_score = min(avg_relevance, 1.0)
        completeness_score = min(num_docs / 5.0, 1.0)  # å‡è®¾5ç¯‡æ–‡æ¡£ä¸ºå®Œæ•´
        novelty_score = 0.7 if num_docs >= 3 else 0.4  # å¤šæ ·æ€§ç®€å•è¯„ä¼°
        authority_score = 0.6  # é»˜è®¤æƒå¨æ€§
        
        overall_score = (relevance_score + completeness_score + novelty_score + authority_score) / 4
        
        # å†³ç­–é€»è¾‘
        if overall_score >= 0.8:
            action = AgenticDecision.PROCEED
        elif overall_score >= 0.6:
            action = AgenticDecision.EXPAND_QUERY
        else:
            action = AgenticDecision.RETRY
        
        return DetailedEvaluation(
            relevance_score=relevance_score,
            completeness_score=completeness_score,
            novelty_score=novelty_score,
            authority_score=authority_score,
            overall_score=overall_score,
            evaluation_reasoning="LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„ä¼°",
            specific_issues=["LLMè¯„ä¼°ä¸å¯ç”¨"],
            improvement_suggestions={
                "query_modification": "å°è¯•æ·»åŠ æ›´å¤šå…³é”®è¯",
                "search_strategy": "ä½¿ç”¨ä¸åŒçš„æ£€ç´¢ç­–ç•¥"
            },
            recommended_action=action,
            confidence_in_decision=0.3
        )


class QueryRefiner:
    """æŸ¥è¯¢ç²¾ç‚¼å™¨ - åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢"""
    
    def __init__(self, llm):
        self.llm = llm
        
        self.query_refinement_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚åŸºäºè¯„ä¼°åé¦ˆå’Œæ”¹è¿›å»ºè®®ï¼Œç”Ÿæˆä¼˜åŒ–åçš„æŸ¥è¯¢ã€‚

ä¼˜åŒ–ç­–ç•¥ï¼š
1. expand_query: æ·»åŠ ç›¸å…³æ¦‚å¿µå’ŒåŒä¹‰è¯ï¼Œæ‰©å¤§æ£€ç´¢èŒƒå›´
2. refine_query: å¢åŠ é™å®šè¯ï¼Œç¼©å°æ£€ç´¢èŒƒå›´æé«˜ç²¾ç¡®åº¦
3. seek_more: ä»ä¸åŒè§’åº¦é‡æ–°è¡¨è¾¾æŸ¥è¯¢
4. retry: ç”¨å®Œå…¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼é‡å†™æŸ¥è¯¢

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "optimized_query": "ä¼˜åŒ–åçš„æŸ¥è¯¢",
    "optimization_strategy": "ä½¿ç”¨çš„ä¼˜åŒ–ç­–ç•¥",
    "added_keywords": ["æ–°å¢å…³é”®è¯1", "æ–°å¢å…³é”®è¯2"],
    "reasoning": "ä¼˜åŒ–ç†ç”±"
}}"""),
            ("human", """åŸå§‹æŸ¥è¯¢ï¼š{original_query}
è¯„ä¼°åé¦ˆï¼š{evaluation_feedback}
æ”¹è¿›å»ºè®®ï¼š{improvement_suggestions}
éœ€è¦æ‰§è¡Œçš„æ“ä½œï¼š{recommended_action}

è¯·ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ã€‚""")
        ])
        
        self.refinement_chain = (
            self.query_refinement_prompt 
            | self.llm 
            | JsonOutputParser()
        )
    
    async def refine_query(
        self,
        original_query: str,
        evaluation: DetailedEvaluation,
        iteration_count: int
    ) -> str:
        """åŸºäºè¯„ä¼°ç»“æœç²¾ç‚¼æŸ¥è¯¢"""
        
        try:
            refinement_result = await self.refinement_chain.ainvoke({
                "original_query": original_query,
                "evaluation_feedback": evaluation.evaluation_reasoning,
                "improvement_suggestions": json.dumps(evaluation.improvement_suggestions, ensure_ascii=False, indent=2),
                "recommended_action": evaluation.recommended_action.value
            })
            
            optimized_query = refinement_result.get("optimized_query", original_query)
            
            logger.debug(f"Query refinement: '{original_query}' -> '{optimized_query}'")
            
            return optimized_query
            
        except Exception as e:
            logger.warning(f"Query refinement failed: {e}")
            
            # ç®€å•çš„å¤‡ç”¨ä¼˜åŒ–é€»è¾‘
            return self._simple_query_refinement(original_query, evaluation, iteration_count)
    
    def _simple_query_refinement(
        self,
        original_query: str,
        evaluation: DetailedEvaluation,
        iteration_count: int
    ) -> str:
        """ç®€å•çš„æŸ¥è¯¢ä¼˜åŒ–é€»è¾‘"""
        
        action = evaluation.recommended_action
        
        if action == AgenticDecision.EXPAND_QUERY:
            # æ·»åŠ ç›¸å…³è¯æ±‡
            related_terms = ["ç›¸å…³", "ç±»ä¼¼", "å¯¹æ¯”", "åˆ†æ"]
            return f"{original_query} {related_terms[iteration_count % len(related_terms)]}"
        
        elif action == AgenticDecision.REFINE_QUERY:
            # æ·»åŠ é™å®šè¯
            constraints = ["å…·ä½“", "è¯¦ç»†", "æ·±å…¥", "æœ€æ–°"]
            return f"{constraints[iteration_count % len(constraints)]} {original_query}"
        
        elif action == AgenticDecision.SEEK_MORE:
            # æ¢ä¸ªè§’åº¦
            perspectives = ["å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "å“ªäº›"]
            return f"{perspectives[iteration_count % len(perspectives)]} {original_query}"
        
        else:
            # é»˜è®¤æ·»åŠ ä¸€äº›é€šç”¨è¯
            return f"{original_query} è¯¦ç»†ä¿¡æ¯"


class EnhancedAgenticRAG:
    """å¢å¼ºçš„æ™ºèƒ½ä½“RAGç³»ç»Ÿ"""
    
    def __init__(
        self,
        hybrid_retriever: HybridRetriever,
        llm,
        config: Dict[str, Any]
    ):
        self.hybrid_retriever = hybrid_retriever
        self.llm = llm
        self.config = config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.evaluator = DetailedEvaluator(llm, config)
        self.query_refiner = QueryRefiner(llm)
        
        # é…ç½®å‚æ•°
        self.max_iterations = config.get('max_agentic_iterations', 3)
        self.min_score_threshold = config.get('min_score_threshold', 0.7)
        self.early_stop_threshold = config.get('early_stop_threshold', 0.9)
        
        logger.info(f"Enhanced Agentic RAG initialized: max_iterations={self.max_iterations}, "
                   f"score_threshold={self.min_score_threshold}")
    
    async def retrieve_with_agency(self, query: str) -> AgenticRetrievalResult:
        """æ‰§è¡Œæ™ºèƒ½ä½“æ£€ç´¢"""
        
        start_time = time.time()
        logger.info(f"ğŸ¤– Starting agentic retrieval for: {query[:50]}...")
        
        # åˆå§‹åŒ–çŠ¶æ€
        current_query = query
        all_iterations = []
        query_evolution = [query]
        strategies_used = set()
        
        final_documents = []
        final_evaluation = None
        success = False
        
        for iteration in range(self.max_iterations):
            logger.debug(f"ğŸ”„ Agentic iteration {iteration + 1}/{self.max_iterations}")
            
            iteration_start = time.time()
            
            try:
                # 1. æ‰§è¡Œæ£€ç´¢
                documents = await self.hybrid_retriever._aget_relevant_documents(current_query)
                
                # è®°å½•ä½¿ç”¨çš„ç­–ç•¥
                if documents and hasattr(documents[0], 'retrieval_metadata'):
                    strategies_used.add(documents[0].retrieval_metadata.retrieval_method)
                
                # 2. è¯„ä¼°æ£€ç´¢ç»“æœ
                evaluation = await self.evaluator.evaluate_retrieval(
                    query=current_query,
                    documents=documents,
                    retrieval_history=all_iterations,
                    retrieval_methods=list(strategies_used)
                )
                
                iteration_time = time.time() - iteration_start
                
                # 3. è®°å½•è¿­ä»£
                iteration_record = RetrievalIteration(
                    iteration_number=iteration + 1,
                    query_used=current_query,
                    documents_retrieved=documents,
                    evaluation=evaluation,
                    action_taken=evaluation.recommended_action,
                    processing_time=iteration_time
                )
                all_iterations.append(iteration_record)
                
                # 4. å†³ç­–
                if (evaluation.recommended_action == AgenticDecision.PROCEED or 
                    evaluation.overall_score >= self.early_stop_threshold):
                    # æˆåŠŸå®Œæˆ
                    final_documents = documents
                    final_evaluation = evaluation
                    success = True
                    logger.success(f"âœ… Agentic retrieval succeeded in {iteration + 1} iterations")
                    break
                
                elif iteration == self.max_iterations - 1:
                    # æœ€åä¸€æ¬¡è¿­ä»£ï¼Œä½¿ç”¨å½“å‰ç»“æœ
                    final_documents = documents
                    final_evaluation = evaluation
                    success = evaluation.overall_score >= self.min_score_threshold
                    if success:
                        logger.success(f"âœ… Agentic retrieval completed with acceptable score")
                    else:
                        logger.warning(f"âš ï¸ Agentic retrieval completed with low score")
                    break
                
                else:
                    # 5. ä¼˜åŒ–æŸ¥è¯¢ç»§ç»­è¿­ä»£
                    optimized_query = await self.query_refiner.refine_query(
                        original_query=query,
                        evaluation=evaluation,
                        iteration_count=iteration
                    )
                    
                    if optimized_query != current_query:
                        current_query = optimized_query
                        query_evolution.append(optimized_query)
                        logger.debug(f"ğŸ”§ Query refined to: {optimized_query}")
                    else:
                        logger.warning("Query refinement produced no change, continuing with current query")
                
            except Exception as e:
                logger.error(f"âŒ Iteration {iteration + 1} failed: {e}")
                
                # å¦‚æœæœ‰ä¹‹å‰çš„ç»“æœï¼Œä½¿ç”¨å®ƒä»¬
                if all_iterations:
                    last_iteration = all_iterations[-1]
                    final_documents = last_iteration.documents_retrieved
                    final_evaluation = last_iteration.evaluation
                    success = False
                    break
                else:
                    # åˆ›å»ºç©ºç»“æœ
                    final_documents = []
                    final_evaluation = self.evaluator._create_fallback_evaluation([])
                    success = False
                    break
        
        total_time = time.time() - start_time
        
        # æ„å»ºç»“æœ
        result = AgenticRetrievalResult(
            final_documents=final_documents,
            all_iterations=all_iterations,
            total_iterations=len(all_iterations),
            success=success,
            final_evaluation=final_evaluation,
            total_time=total_time,
            query_evolution=query_evolution,
            strategies_used=list(strategies_used),
            performance_metrics={
                "avg_iteration_time": total_time / len(all_iterations) if all_iterations else 0,
                "total_documents_evaluated": sum(len(it.documents_retrieved) for it in all_iterations),
                "final_score": final_evaluation.overall_score if final_evaluation else 0.0,
                "iterations_used": len(all_iterations),
                "max_iterations": self.max_iterations
            }
        )
        
        logger.info(f"ğŸ¯ Agentic retrieval completed: {len(final_documents)} final docs, "
                   f"score={final_evaluation.overall_score:.2f}, time={total_time:.2f}s")
        
        return result
    
    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.config.update(new_config)
        self.max_iterations = self.config.get('max_agentic_iterations', 3)
        self.min_score_threshold = self.config.get('min_score_threshold', 0.7)
        self.early_stop_threshold = self.config.get('early_stop_threshold', 0.9)
        
        logger.info(f"Agentic RAG config updated: max_iter={self.max_iterations}, "
                   f"min_score={self.min_score_threshold}, early_stop={self.early_stop_threshold}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ï¼ˆå¯ä»¥æ‰©å±•ä¸ºå®é™…ç»Ÿè®¡ï¼‰"""
        return {
            "config": {
                "max_iterations": self.max_iterations,
                "min_score_threshold": self.min_score_threshold,
                "early_stop_threshold": self.early_stop_threshold
            },
            "components": {
                "evaluator": "DetailedEvaluator",
                "query_refiner": "QueryRefiner",
                "hybrid_retriever": type(self.hybrid_retriever).__name__
            }
        }