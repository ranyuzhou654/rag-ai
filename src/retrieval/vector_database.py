# src/retrieval/vector_database.py
from typing import List, Dict, Optional, Tuple, Any
import numpy as np
from dataclasses import asdict
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from loguru import logger
import uuid
from pathlib import Path
import json
import time

class QdrantVectorDB:
    """
    Qdrantå‘é‡æ•°æ®åº“ç®¡ç†å™¨
    æ ¸å¿ƒåŠŸèƒ½ï¼šé«˜æ•ˆçš„å‘é‡æ£€ç´¢ + æ··åˆæœç´¢èƒ½åŠ›
    """
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        collection_name: str = "ai_papers",
        vector_size: int = 1024,  # BGE-M3çš„å‘é‡ç»´åº¦
        timeout: int = 60 # æ–°å¢è¶…æ—¶å‚æ•°
    ):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.vector_size = vector_size
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        try:
            self.client = QdrantClient(host=host, port=port, timeout=timeout) # åº”ç”¨è¶…æ—¶å‚æ•°
            logger.info(f"æˆåŠŸè¿æ¥åˆ°Qdrant: {host}:{port} (è¶…æ—¶è®¾ç½®ä¸º {timeout}s)")
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            self._ensure_collection_exists()
            
        except Exception as e:
            logger.error(f"è¿æ¥Qdrantå¤±è´¥: {e}")
            raise
    
    def _ensure_collection_exists(self):
        """ç¡®ä¿é›†åˆå­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
                
                # åˆ›å»ºé›†åˆé…ç½®
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.vector_size,
                        distance=Distance.COSINE  # ä½™å¼¦ç›¸ä¼¼åº¦
                    ),
                    # ä¼˜åŒ–é…ç½®
                    optimizers_config=models.OptimizersConfig(
                        default_segment_number=2,  # åˆ†æ®µæ•°
                        max_segment_size=20000,    # æœ€å¤§åˆ†æ®µå¤§å°
                        memmap_threshold=20000,    # å†…å­˜æ˜ å°„é˜ˆå€¼
                        indexing_threshold=10000   # ç´¢å¼•é˜ˆå€¼
                    ),
                    # HNSWç´¢å¼•å‚æ•°ä¼˜åŒ–
                    hnsw_config=models.HnswConfig(
                        m=16,                      # æ¯å±‚è¿æ¥æ•°
                        ef_construct=200,          # æ„å»ºæ—¶æœç´¢å®½åº¦
                        full_scan_threshold=10000  # å…¨æ‰«æé˜ˆå€¼
                    )
                )
                
                logger.info("âœ… é›†åˆåˆ›å»ºæˆåŠŸ")
            else:
                # è·å–é›†åˆä¿¡æ¯
                info = self.client.get_collection(self.collection_name)
                logger.info(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
                logger.info(f"å½“å‰å‘é‡æ•°é‡: {info.points_count}")

        except Exception as e:
            logger.error(f"ç¡®ä¿é›†åˆå­˜åœ¨å¤±è´¥: {e}")

    def add_chunks(self, chunks: List[Dict], batch_size: int = 100) -> bool:
        """
        æ‰¹é‡æ·»åŠ æ–‡æœ¬å—åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨ (åŒ…å«embedding)
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"å¼€å§‹æ·»åŠ  {len(chunks)} ä¸ªå‘é‡åˆ°æ•°æ®åº“...")
        
        total_added = 0
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            points = []
            
            for chunk in batch:
                # æ£€æŸ¥æ˜¯å¦æœ‰embedding
                if 'embedding' not in chunk or chunk['embedding'] is None:
                    logger.warning(f"è·³è¿‡æ— embeddingçš„chunk: {chunk.get('chunk_id', 'unknown')}")
                    continue
                
                # å‡†å¤‡å‘é‡ç‚¹
                try:
                    point = PointStruct(
                        id=str(uuid.uuid4()),  # ç”Ÿæˆå”¯ä¸€ID
                        vector=chunk['embedding'],
                        payload={
                            'chunk_id': chunk['chunk_id'],
                            'source_id': chunk['source_id'],
                            'content': chunk['content'],
                            'semantic_type': chunk.get('semantic_type', 'content'),
                            'metadata': chunk.get('metadata', {}),
                            # æ·»åŠ å…¨æ–‡æ£€ç´¢å­—æ®µ
                            'text_tokens': self._tokenize_for_search(chunk['content'])
                        }
                    )
                    points.append(point)
                    
                except Exception as e:
                    logger.error(f"å‡†å¤‡å‘é‡ç‚¹å¤±è´¥: {e}")
                    continue
            
            if points:
                try:
                    # æ‰¹é‡ä¸Šä¼ 
                    operation_info = self.client.upsert(
                        collection_name=self.collection_name,
                        wait=True,  # ç­‰å¾…æ“ä½œå®Œæˆ
                        points=points
                    )
                    
                    batch_count = len(points)
                    total_added += batch_count
                    logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ·»åŠ  {batch_count} ä¸ªå‘é‡")
                    
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ä¸Šä¼ å¤±è´¥: {e}")
                    continue
        
        # ç»Ÿè®¡ä¿¡æ¯
        collection_info = self.client.get_collection(self.collection_name)
        success_rate = total_added / len(chunks) * 100
        
        logger.info(f"âœ… å‘é‡æ·»åŠ å®Œæˆ!")
        logger.info(f"   æˆåŠŸæ·»åŠ : {total_added}/{len(chunks)} ({success_rate:.1f}%)")
        logger.info(f"   æ•°æ®åº“æ€»å‘é‡æ•°: {collection_info.points_count}")
        
        return success_rate > 90  # æˆåŠŸç‡å¤§äº90%è®¤ä¸ºæˆåŠŸ
    
    def _tokenize_for_search(self, text: str) -> List[str]:
        """ä¸ºå…¨æ–‡æ£€ç´¢å‡†å¤‡token"""
        import re
        
        # ç®€å•çš„åˆ†è¯ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºæ›´ä¸“ä¸šçš„åˆ†è¯å™¨ï¼‰
        tokens = re.findall(r'\b\w{3,}\b', text.lower())
        return list(set(tokens))  # å»é‡
    
    def hybrid_search(
        self,
        query_vector: np.ndarray,
        query_text: str,
        top_k: int = 10,
        vector_weight: float = 0.7,
        text_weight: float = 0.3,
        filter_condition: Optional[Dict] = None
    ) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + æ–‡æœ¬åŒ¹é…
        è¿™æ˜¯å·¥ä¸šçº§RAGç³»ç»Ÿçš„æ ‡å‡†åšæ³•
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°
            vector_weight: å‘é‡æ£€ç´¢æƒé‡
            text_weight: æ–‡æœ¬æ£€ç´¢æƒé‡
            filter_condition: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            start_time = time.time()
            
            # 1. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
            vector_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector.tolist(),
                limit=top_k * 2,  # å¤šæ£€ç´¢ä¸€äº›å€™é€‰
                query_filter=self._build_filter(filter_condition) if filter_condition else None,
                with_payload=True,
                with_vectors=False  # ä¸è¿”å›å‘é‡ä»¥èŠ‚çœå¸¦å®½
            )
            
            # 2. æ–‡æœ¬å…³é”®è¯åŒ¹é…å¾—åˆ†
            query_tokens = set(self._tokenize_for_search(query_text))
            
            results = []
            for hit in vector_results:
                payload = hit.payload
                
                # è®¡ç®—æ–‡æœ¬åŒ¹é…å¾—åˆ†
                content_tokens = set(payload.get('text_tokens', []))
                text_score = len(query_tokens.intersection(content_tokens)) / max(len(query_tokens), 1)
                
                # æ··åˆå¾—åˆ†
                vector_score = hit.score
                hybrid_score = vector_weight * vector_score + text_weight * text_score
                
                result = {
                    'chunk_id': payload['chunk_id'],
                    'source_id': payload['source_id'],
                    'content': payload['content'],
                    'semantic_type': payload.get('semantic_type', 'content'),
                    'metadata': payload.get('metadata', {}),
                    'scores': {
                        'vector_score': float(vector_score),
                        'text_score': text_score,
                        'hybrid_score': hybrid_score
                    }
                }
                results.append(result)
            
            # 3. æŒ‰æ··åˆå¾—åˆ†é‡æ–°æ’åº
            results.sort(key=lambda x: x['scores']['hybrid_score'], reverse=True)
            results = results[:top_k]
            
            search_time = time.time() - start_time
            logger.info(f"æ··åˆæ£€ç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ, è€—æ—¶ {search_time:.3f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _build_filter(self, filter_condition: Dict) -> models.Filter:
        """æ„å»ºæŸ¥è¯¢è¿‡æ»¤å™¨"""
        conditions = []
        
        if 'semantic_type' in filter_condition:
            conditions.append(
                models.FieldCondition(
                    key="semantic_type",
                    match=models.MatchValue(value=filter_condition['semantic_type'])
                )
            )
        
        if 'source_type' in filter_condition:
            conditions.append(
                models.FieldCondition(
                    key="metadata.source",
                    match=models.MatchValue(value=filter_condition['source_type'])
                )
            )
        
        return models.Filter(must=conditions) if conditions else None
    
    def get_collection_stats(self) -> Dict:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            info = self.client.get_collection(self.collection_name)
            
            return {
                'total_points': info.points_count,
                'vector_size': info.config.params.vectors.size,
                'distance_metric': info.config.params.vectors.distance.name,
                'status': info.status.name,
                'optimizer_status': info.optimizer_status
            }
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

class VectorDatabaseManager:
    """å‘é‡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.db = QdrantVectorDB(
            host=config.get('qdrant_host', 'localhost'),
            port=config.get('qdrant_port', 6333),
            collection_name=config.get('collection_name', 'ai_papers'),
            vector_size=config.get('vector_size', 1024),
            timeout=config.get('qdrant_timeout', 120) # ä»é…ç½®è¯»å–æˆ–é»˜è®¤120ç§’
        )
    
    def build_knowledge_base(self, processed_chunks_path: Path) -> bool:
        """
        ä»å¤„ç†å¥½çš„æ–‡æœ¬å—æ„å»ºçŸ¥è¯†åº“
        
        Args:
            processed_chunks_path: å¤„ç†åçš„æ–‡æœ¬å—JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æ„å»ºæˆåŠŸ
        """
        logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†åº“: {processed_chunks_path}")
        
        try:
            # åŠ è½½å¤„ç†åçš„æ•°æ®
            with open(processed_chunks_path, 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            logger.info(f"åŠ è½½åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # è¿‡æ»¤æœ‰æ•ˆçš„chunksï¼ˆå¿…é¡»æœ‰embeddingï¼‰
            valid_chunks = [c for c in chunks if c.get('embedding') is not None]
            logger.info(f"æœ‰æ•ˆæ–‡æœ¬å—: {len(valid_chunks)}/{len(chunks)}")
            
            if not valid_chunks:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—ï¼ˆç¼ºå°‘embeddingï¼‰")
                return False
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            success = self.db.add_chunks(valid_chunks)
            
            if success:
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                stats = self.db.get_collection_stats()
                logger.info("çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼")
                logger.info(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")
                
                return True
            else:
                logger.error("çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æ„å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            return False
    
    def search(
        self,
        query_vector: np.ndarray,
        query_text: str,
        top_k: int = 5,
        **kwargs
    ) -> List[Dict]:
        """æ‰§è¡Œæ£€ç´¢"""
        return self.db.hybrid_search(
            query_vector=query_vector,
            query_text=query_text,
            top_k=top_k,
            **kwargs
        )

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def main():
    from pathlib import Path
    import json
    from sentence_transformers import SentenceTransformer
    
    # é…ç½®
    config = {
        'qdrant_host': 'localhost',
        'qdrant_port': 6333,
        'collection_name': 'ai_papers',
        'vector_size': 1024,  # BGE-M3ç»´åº¦
        'qdrant_timeout': 180 # æµ‹è¯•æ—¶ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶
    }
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    db_manager = VectorDatabaseManager(config)
    
    # æ„å»ºçŸ¥è¯†åº“
    processed_data_path = Path("data/processed/processed_chunks.json")
    
    if processed_data_path.exists():
        logger.info("å¼€å§‹æ„å»ºå‘é‡çŸ¥è¯†åº“...")
        success = db_manager.build_knowledge_base(processed_data_path)
        
        if success:
            print("âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼")
            
            # æµ‹è¯•æ£€ç´¢
            print("\nğŸ” æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
            
            # åŠ è½½embeddingæ¨¡å‹è¿›è¡Œæµ‹è¯•
            embedder = SentenceTransformer('BAAI/bge-m3')
            
            test_queries = [
                "What are the latest developments in large language models?",
                "å¦‚ä½•æ”¹è¿›Transformeræ¨¡å‹çš„æ•ˆç‡ï¼Ÿ",
                "computer vision deep learning advances"
            ]
            
            for query in test_queries:
                print(f"\næŸ¥è¯¢: {query}")
                query_vector = embedder.encode([query], convert_to_numpy=True)[0]
                
                results = db_manager.search(
                    query_vector=query_vector,
                    query_text=query,
                    top_k=3
                )
                
                for i, result in enumerate(results, 1):
                    print(f"  {i}. [{result['semantic_type']}] {result['content'][:100]}...")
                    print(f"     å¾—åˆ†: {result['scores']['hybrid_score']:.3f}")
        
        else:
            print("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶: {processed_data_path}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

