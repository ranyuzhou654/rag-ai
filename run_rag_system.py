# run_rag_system.py - é¡¹ç›®ä¸€é”®è¿è¡Œè„šæœ¬
import asyncio
import argparse
import sys
from pathlib import Path
import json
import time
import subprocess
from datetime import datetime

# åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰åŠ è½½é…ç½®ï¼Œä»¥è®¾ç½®ç¯å¢ƒå˜é‡
try:
    from configs.config import config
except ImportError:
    print("æ— æ³•å¯¼å…¥é…ç½®ï¼Œè¯·ç¡®ä¿configs/config.pyå­˜åœ¨")
    sys.exit(1)
    
from loguru import logger
from huggingface_hub import login

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# è®¾ç½®æ—¥å¿—
log_file = config.LOG_DIR / f"rag_system_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
log_file.parent.mkdir(exist_ok=True)
logger.add(log_file, rotation="1 day", retention="30 days")

def hf_login():
    """ä½¿ç”¨é…ç½®ä¸­çš„ä»¤ç‰Œç™»å½•Hugging Face"""
    token = config.HUGGING_FACE_TOKEN
    if token:
        try:
            login(token=token, add_to_git_credential=False)
            logger.success("âœ… Hugging Face ç™»å½•æˆåŠŸ")
        except Exception as e:
            logger.warning(f"ğŸ¤” HFç™»å½•å¤±è´¥: {e}")
    else:
        logger.info("ğŸ’¡ æœªé…ç½®HFä»¤ç‰Œï¼Œéƒ¨åˆ†æ¨¡å‹å¯èƒ½å—é™ã€‚")

class RAGSystemRunner:
    """RAGç³»ç»Ÿè¿è¡Œå™¨ - ç®¡ç†æ•´ä¸ªé¡¹ç›®çš„ç”Ÿå‘½å‘¨æœŸ"""
    
    def __init__(self):
        self.project_root = project_root
        self.data_dir = config.DATA_DIR
        self.config_file = self.project_root / "configs" / "config.py"
        
        # æ£€æŸ¥é¡¹ç›®ç»“æ„
        self._check_project_structure()
    
    def _check_project_structure(self):
        """æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„"""
        required_dirs = [
            "data/raw",
            "data/processed", 
            "src/data_ingestion",
            "src/processing",
            "src/retrieval",
            "src/generation",
            "logs",
            "configs"
        ]
        
        for dir_path in required_dirs:
            full_path = self.project_root / dir_path
            if not full_path.exists():
                logger.warning(f"åˆ›å»ºç¼ºå¤±ç›®å½•: {dir_path}")
                full_path.mkdir(parents=True, exist_ok=True)
    
    def check_environment(self) -> bool:
        """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
        logger.info("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
        
        success = True
        
        # 1. æ£€æŸ¥Pythonç‰ˆæœ¬
        python_version = sys.version_info
        if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
            logger.error(f"âŒ Pythonç‰ˆæœ¬è¿‡ä½: {python_version.major}.{python_version.minor}")
            success = False
        else:
            logger.info(f"âœ… Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}")
        
        # 2. æ£€æŸ¥å…³é”®ä¾èµ–
        required_packages = [
            'torch',
            'transformers', 
            'sentence_transformers',
            'qdrant_client',
            'streamlit',
            'pymupdf4llm'
        ]
        
        for package in required_packages:
            try:
                __import__(package)
                logger.info(f"âœ… {package} å·²å®‰è£…")
            except ImportError:
                logger.error(f"âŒ {package} æœªå®‰è£…")
                success = False
        
        # 3. æ£€æŸ¥QdrantæœåŠ¡
        try:
            from qdrant_client import QdrantClient
            client = QdrantClient(host="localhost", port=6333)
            collections = client.get_collections()
            logger.info("âœ… QdrantæœåŠ¡è¿æ¥æ­£å¸¸")
        except Exception as e:
            logger.warning(f"âš ï¸ QdrantæœåŠ¡è¿æ¥å¤±è´¥: {e}")
            logger.info("æç¤º: è¯·ç¡®ä¿Dockerä¸­çš„QdrantæœåŠ¡æ­£åœ¨è¿è¡Œ")
            success = False
        
        return success

    async def run_full_pipeline(self, args):
        """è¿è¡Œå®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹...")
        start_time = time.time()
        
        # 1. ç¯å¢ƒæ£€æŸ¥ (å¯ä»¥æ ¹æ®éœ€è¦ä¿ç•™æˆ–ç®€åŒ–)
        
        # 2. æ•°æ®æ”¶é›†
        if not args.skip_collect:
            if not await self.collect_data(args.max_papers, args.days_back):
                logger.error("âŒ æ•°æ®æ”¶é›†å¤±è´¥")
                return False
        
        # 3. æ•°æ®å¤„ç†
        if not args.skip_process:
            if not await self.process_data():
                logger.error("âŒ æ•°æ®å¤„ç†å¤±è´¥") 
                return False
        
        # 4. æ„å»ºçŸ¥è¯†åº“
        if not args.skip_build:
            if not self.build_knowledge_base():
                logger.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return False

        logger.info(f"âœ… RAGç³»ç»Ÿå‡†å¤‡å®Œæˆ! æ€»è€—æ—¶: {time.time() - start_time:.1f}ç§’")
        
        # 5. å¯åŠ¨å‰ç«¯
        if not args.no_frontend:
            self.launch_frontend(args.port)
        
        return True
    
    async def collect_data(self, max_papers: int = 50, days_back: int = 7) -> bool:
        """æ•°æ®æ”¶é›†æ­¥éª¤"""
        logger.info("ğŸ“¥ å¼€å§‹æ•°æ®æ”¶é›†...")
        
        try:
            from src.data_ingestion.multi_source_collector import MultiSourceCollector
            
            # åˆå§‹åŒ–æ”¶é›†å™¨
            collector = MultiSourceCollector(self.data_dir / "raw")
            
            # æ”¶é›†æ•°æ®
            all_data = await collector.collect_all(days_back=days_back)
            
            if not all_data:
                logger.error("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
                return False
            
            logger.success(f"âœ… æ•°æ®æ”¶é›†å®Œæˆ: {len(all_data)} æ¡æ•°æ®å·²ç”±æ”¶é›†å™¨ä¿å­˜ã€‚")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return False
    
    async def process_data(self) -> bool:
        """æ•°æ®å¤„ç†æ­¥éª¤"""
        logger.info("âš™ï¸ å¼€å§‹æ•°æ®å¤„ç†...")
        
        try:
            from src.processing.text_processor import TextProcessor
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®
            input_file = self.data_dir / "raw" / "raw_collected_data.json"
            if not input_file.exists():
                logger.error(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ•°æ®æ–‡ä»¶: {input_file}")
                return False
            
            # åŠ è½½æ•°æ®
            with open(input_file, 'r', encoding='utf-8') as f:
                documents = json.load(f)
            
            logger.info(f"ğŸ“„ åŠ è½½åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            
            # åˆå§‹åŒ–å¤„ç†å™¨
            config = {
                'chunk_size': 512,
                'overlap': 50,
                'embedding_model': 'BAAI/bge-m3',
                'device': 'auto'
            }
            
            processor = TextProcessor(config)
            
            # å¤„ç†æ–‡æ¡£
            chunks = processor.process_documents(documents)
            
            if not chunks:
                logger.error("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„æ–‡æœ¬å—")
                return False
            
            # ä¿å­˜ç»“æœ
            output_file = self.data_dir / "processed" / "processed_chunks.json"
            processor.save_processed_data(chunks, output_file)
            
            logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—å·²ä¿å­˜è‡³ {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return False
    
    def build_knowledge_base(self) -> bool:
        """æ„å»ºå‘é‡çŸ¥è¯†åº“"""
        logger.info("ğŸ—„ï¸ å¼€å§‹æ„å»ºå‘é‡çŸ¥è¯†åº“...")
        
        try:
            from src.retrieval.vector_database import VectorDatabaseManager
            
            # æ£€æŸ¥å¤„ç†åçš„æ•°æ®
            processed_file = self.data_dir / "processed" / "processed_chunks.json"
            if not processed_file.exists():
                logger.error(f"âŒ æ‰¾ä¸åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶: {processed_file}")
                return False
            
            # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
            config = {
                'qdrant_host': 'localhost',
                'qdrant_port': 6333,
                'collection_name': 'ai_papers',
                'vector_size': 1024
            }
            
            db_manager = VectorDatabaseManager(config)
            
            # æ„å»ºçŸ¥è¯†åº“
            success = db_manager.build_knowledge_base(processed_file)
            
            if success:
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = db_manager.db.get_collection_stats()
                logger.info(f"âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸ!")
                logger.info(f"   æ€»å‘é‡æ•°: {stats.get('total_points', 0)}")
                logger.info(f"   å‘é‡ç»´åº¦: {stats.get('vector_size', 0)}")
                return True
            else:
                logger.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            return False
    
    def test_system(self) -> bool:
        """æµ‹è¯•ç³»ç»ŸåŠŸèƒ½"""
        logger.info("ğŸ§ª å¼€å§‹ç³»ç»Ÿæµ‹è¯•...")
        
        try:
            from src.generation.rag_generator import RAGSystem
            from src.retrieval.vector_database import VectorDatabaseManager
            
            # åˆå§‹åŒ–ç³»ç»Ÿ
            config = {
                'embedding_model': 'BAAI/bge-m3',
                'llm_model': 'Qwen/Qwen2-7B-Instruct',
                'device': 'auto',
                'qdrant_host': 'localhost',
                'qdrant_port': 6333,
                'collection_name': 'ai_papers'
            }
            
            rag_system = RAGSystem(config)
            db_manager = VectorDatabaseManager(config)
            rag_system.set_database(db_manager)
            
            # æµ‹è¯•æŸ¥è¯¢
            test_queries = [
                "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿ",
                "How do attention mechanisms work?"
            ]
            
            success_count = 0
            
            for query in test_queries:
                try:
                    logger.info(f"æµ‹è¯•æŸ¥è¯¢: {query}")
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    result = loop.run_until_complete(
                        rag_system.generate_answer(query, top_k=3)
                    )
                    
                    loop.close()
                    
                    if result and result.answer:
                        logger.info(f"âœ… æŸ¥è¯¢æˆåŠŸ - ç½®ä¿¡åº¦: {result.confidence:.2f}")
                        success_count += 1
                    else:
                        logger.warning(f"âš ï¸ æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
                        
                except Exception as e:
                    logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            
            success_rate = success_count / len(test_queries)
            if success_rate >= 0.5:
                logger.info(f"âœ… ç³»ç»Ÿæµ‹è¯•é€šè¿‡ - æˆåŠŸç‡: {success_rate:.1%}")
                return True
            else:
                logger.error(f"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥ - æˆåŠŸç‡: {success_rate:.1%}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def launch_frontend(self, port: int = 8501):
        """å¯åŠ¨Streamlitå‰ç«¯"""
        logger.info(f"ğŸš€ å¯åŠ¨å‰ç«¯ç•Œé¢ (ç«¯å£: {port})...")
        
        try:
            app_file = self.project_root / "app.py"
            if not app_file.exists():
                logger.error(f"âŒ æ‰¾ä¸åˆ°å‰ç«¯åº”ç”¨æ–‡ä»¶: {app_file}")
                return False
            
            # å¯åŠ¨Streamlit
            cmd = [
                sys.executable, "-m", "streamlit", "run",
                str(app_file),
                "--server.port", str(port),
                "--server.address", "localhost"
            ]
            
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # åœ¨æ–°è¿›ç¨‹ä¸­å¯åŠ¨
            process = subprocess.Popen(
                cmd,
                cwd=self.project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´æ£€æŸ¥æ˜¯å¦å¯åŠ¨æˆåŠŸ
            time.sleep(3)
            
            if process.poll() is None:  # è¿›ç¨‹ä»åœ¨è¿è¡Œ
                logger.info(f"âœ… å‰ç«¯å¯åŠ¨æˆåŠŸ!")
                logger.info(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:{port}")
                
                try:
                    process.wait()  # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                except KeyboardInterrupt:
                    logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
                    process.terminate()
                    process.wait()
                
                return True
            else:
                stdout, stderr = process.communicate()
                logger.error(f"âŒ å‰ç«¯å¯åŠ¨å¤±è´¥")
                logger.error(f"stdout: {stdout}")
                logger.error(f"stderr: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å‰ç«¯å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    async def run_full_pipeline(self, args):
        """è¿è¡Œå®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹...")
        
        start_time = time.time()
        
        # 1. ç¯å¢ƒæ£€æŸ¥
        if not args.skip_check and not self.check_environment():
            logger.error("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ä¿®å¤é—®é¢˜åé‡è¯•")
            return False
        
        # 2. æ•°æ®æ”¶é›†ï¼ˆå¯é€‰ï¼‰
        if not args.skip_collect:
            if not await self.collect_data(args.max_papers, args.days_back):
                logger.error("âŒ æ•°æ®æ”¶é›†å¤±è´¥")
                return False
        
        # 3. æ•°æ®å¤„ç†
        if not args.skip_process:
            if not await self.process_data():
                logger.error("âŒ æ•°æ®å¤„ç†å¤±è´¥") 
                return False
        
        # 4. æ„å»ºçŸ¥è¯†åº“
        if not args.skip_build:
            if not self.build_knowledge_base():
                logger.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return False
        
        # 5. ç³»ç»Ÿæµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        if args.test:
            if not self.test_system():
                logger.warning("âš ï¸ ç³»ç»Ÿæµ‹è¯•å¤±è´¥ï¼Œä½†ä»å¯ç»§ç»­å¯åŠ¨å‰ç«¯")
        
        total_time = time.time() - start_time
        logger.info(f"âœ… RAGç³»ç»Ÿå‡†å¤‡å®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        # 6. å¯åŠ¨å‰ç«¯
        if not args.no_frontend:
            self.launch_frontend(args.port)
        
        return True

def main():
    """ä¸»å‡½æ•°"""

    hf_login()
    
    parser = argparse.ArgumentParser(
        description="RAG AIæŠ€æœ¯èµ„è®¯åˆ†æå¸ˆç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_rag_system.py                    # è¿è¡Œå®Œæ•´æµç¨‹
  python run_rag_system.py --quick           # å¿«é€Ÿå¯åŠ¨(è·³è¿‡æ•°æ®æ”¶é›†)
  python run_rag_system.py --frontend-only   # ä»…å¯åŠ¨å‰ç«¯
  python run_rag_system.py --test            # è¿è¡Œç³»ç»Ÿæµ‹è¯•
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--port', type=int, default=8501, help='å‰ç«¯ç«¯å£å·')
    parser.add_argument('--max-papers', type=int, default=50, help='æœ€å¤§è®ºæ–‡æ”¶é›†æ•°é‡')
    parser.add_argument('--days-back', type=int, default=7, help='æ”¶é›†æœ€è¿‘å‡ å¤©çš„æ•°æ®')
    
    # æµç¨‹æ§åˆ¶
    parser.add_argument('--skip-check', action='store_true', help='è·³è¿‡ç¯å¢ƒæ£€æŸ¥')
    parser.add_argument('--skip-collect', action='store_true', help='è·³è¿‡æ•°æ®æ”¶é›†')
    parser.add_argument('--skip-process', action='store_true', help='è·³è¿‡æ•°æ®å¤„ç†')
    parser.add_argument('--skip-build', action='store_true', help='è·³è¿‡çŸ¥è¯†åº“æ„å»º')
    parser.add_argument('--no-frontend', action='store_true', help='ä¸å¯åŠ¨å‰ç«¯')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œç³»ç»Ÿæµ‹è¯•')
    
    # ä¾¿æ·é€‰é¡¹
    parser.add_argument('--quick', action='store_true', 
                       help='å¿«é€Ÿå¯åŠ¨(è·³è¿‡æ•°æ®æ”¶é›†ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®)')
    parser.add_argument('--frontend-only', action='store_true',
                       help='ä»…å¯åŠ¨å‰ç«¯ç•Œé¢')
    
    args = parser.parse_args()
    
    # å¤„ç†ä¾¿æ·é€‰é¡¹
    if args.quick:
        args.skip_collect = True
        
    if args.frontend_only:
        args.skip_check = True
        args.skip_collect = True
        args.skip_process = True
        args.skip_build = True
    
    # åˆå§‹åŒ–è¿è¡Œå™¨
    runner = RAGSystemRunner()
    
    print("""
ğŸ¤– ===================================================
   RAG AIæŠ€æœ¯èµ„è®¯åˆ†æå¸ˆç³»ç»Ÿ
   Real-time AI Tech Info Analyst with RAG
ğŸ¤– ===================================================
    """)
    
    try:
        if args.frontend_only:
            # ä»…å¯åŠ¨å‰ç«¯
            runner.launch_frontend(args.port)
        else:
            # è¿è¡Œå®Œæ•´æµç¨‹
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            success = loop.run_until_complete(runner.run_full_pipeline(args))
            loop.close()
            
            if not success:
                print("âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯")
                sys.exit(1)
            
    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç³»ç»Ÿé€€å‡º")
        sys.exit(0)
    except Exception as e:
        logger.exception(f"ç³»ç»Ÿè¿è¡Œå¼‚å¸¸: {e}")
        print(f"âŒ ç³»ç»Ÿå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()